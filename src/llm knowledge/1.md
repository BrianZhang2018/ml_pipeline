1. Training Arguments (setup for learning)
   ├─ num_train_epochs → how many times dataset is seen
   ├─ batch_size → samples per update
   ├─ learning_rate → size of weight update
   ├─ warmup_steps → gradual ramp-up
   ├─ weight_decay → avoid overfitting
   ├─ eval/save_strategy → when to evaluate & save
   └─ logging_dir/steps → monitor progress

------------------------------------------------------------

2. Machine Learning Basics
   ├─ Labels → ground truth answers
   ├─ Logits → raw model scores
   ├─ Softmax → converts logits → probabilities
   └─ Prediction → choose most likely class/token

------------------------------------------------------------

3. Neural Network Engine (Transformer)
   ├─ Embeddings → convert tokens → vectors
   ├─ Transformer Block (repeated many times)
   │     ├─ Multi-Head Attention
   │     │    • Each head looks at different relations
   │     │    • "Attend" = focus strength between words
   │     ├─ Feed-Forward Network (FFN)
   │     └─ Add & Norm (stability)
   └─ Output = contextualized hidden states

------------------------------------------------------------

4. Classification Head
   ├─ Linear Layer (W·h + b) → logits
   ├─ Softmax → probabilities
   └─ Final prediction (Positive/Negative, etc.)

------------------------------------------------------------

5. Fine-Tuning Approaches
   ├─ Full Fine-Tuning
   │     • Update all Transformer + head
   │     • Expensive but powerful
   ├─ Feature Extraction
   │     • Freeze Transformer
   │     • Train only head
   │     • Cheap but less flexible
   └─ Purpose → adapt pretrained model to task

------------------------------------------------------------

6. Efficient Fine-Tuning
   ├─ LoRA
   │     • Insert small trainable matrices in attention
   │     • Freeze big weights, train only adapters
   │     • Saves memory/compute
   └─ LoRAX
         • Manages multiple adapters
         • Swap tasks without retraining

------------------------------------------------------------

7. Inference (using trained model)
   ├─ Prompt → Tokenizer → tokens
   ├─ Tokens → Transformer (engine)
   ├─ Hidden states → Output layer → logits
   ├─ Softmax → probabilities
   └─ Decoding → pick next token

------------------------------------------------------------

8. Step-by-Step Generation
   ├─ Autoregressive (default)
   │     • Predict 1 token at a time
   │     • Loop until end
   ├─ Accelerations
   │     • KV cache, speculative decoding, multi-token heads
   └─ Alternatives
         • Non-autoregressive generation (parallel, refine later)

