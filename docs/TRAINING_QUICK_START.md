# Training Quick Start Guide

A fast-track guide to understanding and running model training in our pipeline.

## ğŸš€ Quick Overview

Our training pipeline transforms pre-trained models into domain-specific classifiers, improving prediction confidence from ~0.53 to ~0.85+ through fine-tuning on movie review data.

## ğŸ“‹ Prerequisites

1. Docker installed and running
2. ML Pipeline Docker image built:
   ```bash
   cd /Users/bz/ai/ml_pipeline
   docker build -t ml_pipeline .
   ```

## âš¡ Quick Start (4 Minutes)

### Step 1: Validate Pipeline
```bash
# Run quick training demo with synthetic data
docker run -it --rm -v $(pwd):/workspace ml_pipeline \
    bash -c "cd /workspace && python quick_train_demo.py"
```

**Expected Output:**
```
ğŸš€ STARTING QUICK TRAINING DEMO
ğŸ“Š STEP 1: Creating synthetic movie review data...
ğŸ”¤ STEP 2: Tokenizing datasets...
ğŸ§  STEP 3: Building model...
ğŸ¯ STEP 4: Initializing trainer...
ğŸš€ STEP 5: Starting training...
ğŸ“ˆ STEP 6: Evaluating model...
ğŸ’¾ STEP 7: Saving trained model...
ğŸ§ª STEP 8: Testing prediction...

ğŸ‰ QUICK TRAINING DEMO COMPLETED!
Final F1 Score: 1.0000
Test: 'This movie was fantastic and amazing!' â†’ positive (0.999)
```

### Step 2: Compare Performance
```bash
# Compare trained vs pre-trained models
docker run -it --rm -v $(pwd):/workspace ml_pipeline \
    bash -c "cd /workspace && python compare_models.py --trained_model ./quick_trained_model --examples"
```

## ğŸ¯ Training Options

### Option A: Quick Demo (Recommended for validation)
- **Data**: 100 synthetic movie reviews
- **Time**: ~4 minutes
- **Purpose**: Validate training pipeline
- **Metrics**: Perfect scores (accuracy: 1.0, F1: 1.0)

### Option B: Test Run (Limited real data)
```bash
docker run -it --rm -v $(pwd):/workspace ml_pipeline \
    bash -c "cd /workspace && python train_model.py --test_run"
```
- **Data**: 1000 real IMDB reviews
- **Time**: ~15 minutes
- **Purpose**: Realistic testing

### Option C: Full Production Training
```bash
docker run -it --rm -v $(pwd):/workspace ml_pipeline \
    bash -c "cd /workspace && python train_model.py --epochs 3 --batch_size 16"
```
- **Data**: 25K IMDB reviews
- **Time**: 30-60 minutes
- **Purpose**: Production model

## ğŸ“Š Understanding Results

### Before Training (Pre-trained Model)
```
Text: "This movie was amazing!"
Prediction: positive (confidence: 0.536)  # Low confidence
```

### After Training (Fine-tuned Model)
```
Text: "This movie was amazing!"
Prediction: positive (confidence: 0.987)  # High confidence
```

## ğŸ”§ Training Process Flow

```
Raw Text â†’ Preprocessing â†’ Tokenization â†’ Model Loading â†’ Fine-tuning â†’ Evaluation â†’ Saving
```

1. **Data Preparation**: Clean and tokenize movie reviews
2. **Model Setup**: Load pre-trained DistilBERT with classification head
3. **Training**: Fine-tune on sentiment classification task
4. **Evaluation**: Measure accuracy, F1, precision, recall
5. **Saving**: Persist trained model for deployment

## ğŸ“ Output Files

After training, you'll find:
```
./quick_trained_model/          # Quick demo output
â”œâ”€â”€ config.json                 # Model configuration
â”œâ”€â”€ pytorch_model.bin           # Trained weights
â”œâ”€â”€ tokenizer_config.json       # Tokenizer settings
â”œâ”€â”€ tokenizer.json              # Tokenizer files
â””â”€â”€ vocab.txt                   # Vocabulary

./trained_models/               # Full training output
â””â”€â”€ distilbert-base-uncased_imdb_final/
    â”œâ”€â”€ config.json
    â”œâ”€â”€ pytorch_model.bin
    â””â”€â”€ ...
```

## ğŸš¨ Troubleshooting

### Error: "accelerate library missing"
**Solution**: Rebuild Docker image (dependency now included in requirements.txt)

### Error: "Network timeout during dataset download"
**Solution**: Use quick demo mode with synthetic data

### Error: "CUDA out of memory"
**Solution**: Reduce batch size or use CPU-only training

## ğŸ“š Detailed Documentation

- [ğŸ“– Complete Training Guide](training_process_guide.md)
- [ğŸ—ï¸ Architecture Diagrams](training_architecture_diagrams.md)
- [âš™ï¸ Configuration Options](../src/models/model_trainer.py)

## ğŸ¯ Next Steps

1. **Start Here**: Run quick demo to validate pipeline
2. **Test Real Data**: Use `--test_run` for realistic testing  
3. **Production Training**: Run full training for deployment
4. **Deploy Model**: Use trained model in API endpoints
5. **Compare Performance**: Measure improvement vs baseline

## ğŸ’¡ Pro Tips

- Always start with quick demo for fast validation
- Use synthetic data to avoid network bottlenecks
- Monitor Docker build logs for dependency issues
- Save training outputs for later comparison
- Check evaluation metrics to ensure training success

**Ready to train? Start with the quick demo above! ğŸš€**